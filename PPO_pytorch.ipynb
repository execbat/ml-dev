{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81335d3f-1dfd-44ae-9aca-59b00f68be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608985a4-d92e-4e53-bef2-cf46b4cc4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_obs, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, n_actions),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_obs):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_obs, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0928ca6d-54f0-4edb-8632-c8808f3a7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma, lam):\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    values = values + [torch.tensor(0.0)]\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i+1] * (1.0 - dones[i]) - values[i]\n",
    "        gae = delta + gamma * lam * (1.0 - dones[i]) * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    return returns        \n",
    "\n",
    "def collect_samples(env, actor, critic, gamma, lam):\n",
    "    states, actions, dones, log_probs, action_probs_collect = [], [], [], [], []\n",
    "\n",
    "    # play episode\n",
    "    state, _ = env.reset()\n",
    "    rewards, values = [], []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        state_tensor = torch.tensor(state, dtype = torch.float32)\n",
    "        states.append(state_tensor)\n",
    "        action_probs = actor(state_tensor.unsqueeze(0))\n",
    "        action_probs_collect.append(action_probs.detach())\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        actions.append(action.item())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        value = critic(state_tensor.unsqueeze(0)).squeeze(0)\n",
    "        values.append(value)\n",
    "\n",
    "        state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "\n",
    "    # aggregate collected data\n",
    "    total_reward = sum(rewards)\n",
    "    returns = compute_gae(rewards, values, dones, gamma, lam)\n",
    "    \n",
    "    advantages = torch.tensor(returns, dtype = torch.float32) - torch.tensor(values, dtype = torch.float32)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)    \n",
    "\n",
    "    # Convert to tensors\n",
    "    states = torch.stack(states)\n",
    "    actions = torch.tensor(actions)\n",
    "    old_log_probs = torch.stack(log_probs).detach()\n",
    "    old_probs = torch.stack(action_probs_collect).detach()\n",
    "    returns = torch.tensor(returns, dtype = torch.float32)\n",
    "    advantages = advantages.detach()\n",
    "\n",
    "    return states, actions, old_log_probs, old_probs, returns, advantages, total_reward           \n",
    "\n",
    "def train(env, actor, critic, gamma = 0.99, lam = 0.95, clip_eps = 0.2, lr = 0.001, entropy_coef = 0.001,  episodes = 1000, update_epochs = 5, batch_size = 64, print_reward_every = 1000.0):\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr = lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr = lr)\n",
    "\n",
    "    reward_collection = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        states, actions, old_log_probs, old_probs, returns, advantages, total_reward = collect_samples(env, actor, critic, gamma, lam)\n",
    "        reward_collection.append(total_reward)\n",
    "        \n",
    "        # cyclical policy update using collected data\n",
    "        for _ in range(update_epochs):\n",
    "            if states.shape[0] < batch_size:\n",
    "                idx = torch.randperm(states.shape[0])\n",
    "            else:\n",
    "                idx = torch.randperm(batch_size)\n",
    "                \n",
    "            batch_states = states[idx]\n",
    "            batch_actions = actions[idx]\n",
    "            batch_old_log_probs = old_log_probs[idx]\n",
    "            batch_old_probs = old_probs[idx]\n",
    "            batch_returns = returns[idx]\n",
    "            batch_advantages = advantages[idx]\n",
    "\n",
    "            action_probs = actor(batch_states)\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            new_log_probs = dist.log_prob(batch_actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "                \n",
    "            ratio = (new_log_probs - batch_old_log_probs).exp()\n",
    "            \n",
    "            surr1 = ratio * batch_advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * batch_advantages\n",
    "            actor_loss = - torch.min(surr1, surr2).mean() - entropy_coef * entropy\n",
    "\n",
    "            critic_loss = nn.MSELoss()(critic(batch_states).squeeze(-1), batch_returns)\n",
    "            \n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "        if episode % print_reward_every == 0 and episode > 0:\n",
    "            print(f\"Episode {episode}, mean reward {sum(reward_collection[-10:]) / 10}\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38831a0-aa4a-492c-8aec-4828f17e0d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, mean reward 28.9\n",
      "Episode 200, mean reward 41.8\n",
      "Episode 300, mean reward 57.9\n",
      "Episode 400, mean reward 57.3\n",
      "Episode 500, mean reward 80.8\n",
      "Episode 600, mean reward 392.1\n",
      "Episode 700, mean reward 369.6\n",
      "Episode 800, mean reward 500.0\n",
      "Episode 900, mean reward 454.7\n",
      "Episode 1000, mean reward 498.8\n",
      "Episode 1100, mean reward 444.1\n",
      "Episode 1200, mean reward 500.0\n",
      "Episode 1300, mean reward 476.7\n",
      "Episode 1400, mean reward 492.1\n",
      "Episode 1500, mean reward 500.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m actor \u001b[38;5;241m=\u001b[39m Actor(n_obs, n_actions)\n\u001b[1;32m      7\u001b[0m critic \u001b[38;5;241m=\u001b[39m Critic(n_obs)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_eps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropy_coef\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_reward_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 73\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, actor, critic, gamma, lam, clip_eps, lr, entropy_coef, episodes, update_epochs, batch_size, print_reward_every)\u001b[0m\n\u001b[1;32m     70\u001b[0m reward_collection \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m---> 73\u001b[0m     states, actions, old_log_probs, old_probs, returns, advantages, total_reward \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     reward_collection\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(update_epochs):\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# for i in range(0, len(states), batch_size):\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m#     idx = slice(i, i + batch_size)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m, in \u001b[0;36mcollect_samples\u001b[0;34m(env, actor, critic, gamma, lam)\u001b[0m\n\u001b[1;32m     49\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(rewards)\n\u001b[1;32m     50\u001b[0m returns \u001b[38;5;241m=\u001b[39m compute_gae(rewards, values, dones, gamma, lam)\n\u001b[0;32m---> 52\u001b[0m advantages \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(values, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     53\u001b[0m advantages \u001b[38;5;241m=\u001b[39m (advantages \u001b[38;5;241m-\u001b[39m advantages\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m (advantages\u001b[38;5;241m.\u001b[39mstd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Convert to tensors\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    n_obs = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    actor = Actor(n_obs, n_actions)\n",
    "    critic = Critic(n_obs)\n",
    "\n",
    "    train(env, actor, critic, gamma = 0.99, lam = 0.95, clip_eps = 0.3, lr = 0.003, entropy_coef = 0.0001,  episodes = 2500, update_epochs = 5, batch_size = 128, print_reward_every = 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8152ed4-bcc7-4234-98cf-0e8424d8e153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9.18_pytorch_gpu",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
