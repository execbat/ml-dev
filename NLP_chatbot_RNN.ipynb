{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRT_4fzVVOsQ"
   },
   "source": [
    "# Implementation of seq2seq Encoder-Decoder\n",
    "# Chat Bot based on Bidirectional RNN architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKuBDSzEV1wL"
   },
   "source": [
    "### import of libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "2ew7HTbPpCJH",
    "outputId": "91b5c5bc-96cf-432e-8555-3a348cf073dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 19:24:56.883164: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-11 19:24:56.883195: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-11 19:24:56.883200: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-11 19:24:56.887631: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import codecs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 19:25:00.879043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-11 19:25:00.900425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-11 19:25:00.904349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# MAKE CuDNN available for computations\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRJ_L-W8V8Ir"
   },
   "source": [
    "### Download the dataset\n",
    "\n",
    "I will use Cornell Movie-Dialogs Corpus for teaching this chat bot.\n",
    "This corpus contains a metadata-rich collection of fictional conversations extracted from raw movie scripts:\n",
    "\n",
    "- 220,579 conversational exchanges between 10,292 pairs of movie characters\n",
    "- involves 9,035 characters from 617 movies\n",
    "- in total 304,713 utterances\n",
    "- movie metadata included:\n",
    "\t- genres\n",
    "\t- release year\n",
    "\t- IMDB rating\n",
    "\t- number of IMDB votes\n",
    "\t- IMDB rating\n",
    "- character metadata included:\n",
    "\t- gender (for 3,774 characters)\n",
    "\t- position on movie credits (3,321 characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the .txt tables into Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_np_arr(filename):\n",
    "# filename = './ChatBot_Dataset/movie_conversations.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip().split(\" +++$+++ \") for line in file]\n",
    "\n",
    "    return np.array(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open conversations to DataFrame\n",
    "- movie_conversations.txt\n",
    "\t- the structure of the conversations\n",
    "\t- fields\n",
    "\t\t- characterID of the first character involved in the conversation\n",
    "\t\t- characterID of the second character involved in the conversation\n",
    "\t\t- movieID of the movie in which the conversation occurred\n",
    "\t\t- list of the utterances that make the conversation, in chronological \n",
    "\t\t\torder: ['lineID1','lineID2',É,'lineIDN']\n",
    "\t\t\thas to be matched with movie_lines.txt to reconstruct the actual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>characterID_1</th>\n",
       "      <th>characterID_2</th>\n",
       "      <th>movieID</th>\n",
       "      <th>conversation_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L194', 'L195', 'L196', 'L197']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L198', 'L199']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L200', 'L201', 'L202', 'L203']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L204', 'L205', 'L206']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['L207', 'L208']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  characterID_1 characterID_2 movieID                 conversation_tags\n",
       "0            u0            u2      m0  ['L194', 'L195', 'L196', 'L197']\n",
       "1            u0            u2      m0                  ['L198', 'L199']\n",
       "2            u0            u2      m0  ['L200', 'L201', 'L202', 'L203']\n",
       "3            u0            u2      m0          ['L204', 'L205', 'L206']\n",
       "4            u0            u2      m0                  ['L207', 'L208']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conversations = pd.DataFrame(file_to_np_arr('./ChatBot_Dataset/movie_conversations.txt'),\n",
    "                                columns = ['characterID_1', 'characterID_2', 'movieID', 'conversation_tags'])\n",
    "df_conversations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open movie lines to DataFrame\n",
    "- movie_lines.txt\n",
    "\t- contains the actual text of each utterance\n",
    "\t- fields:\n",
    "\t\t- lineID\n",
    "\t\t- characterID (who uttered this phrase)\n",
    "\t\t- movieID\n",
    "\t\t- character name\n",
    "\t\t- text of the utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lineID</th>\n",
       "      <th>characterID</th>\n",
       "      <th>movieID</th>\n",
       "      <th>character name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lineID characterID movieID character name          text\n",
       "0  L1045          u0      m0         BIANCA  They do not!\n",
       "1  L1044          u2      m0        CAMERON   They do to!\n",
       "2   L985          u0      m0         BIANCA    I hope so.\n",
       "3   L984          u2      m0        CAMERON     She okay?\n",
       "4   L925          u0      m0         BIANCA     Let's go."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD MOVIE LINES\n",
    "filename = './ChatBot_Dataset/movie_lines.txt'\n",
    "counter = 0\n",
    "lines = list()\n",
    "with open(filename) as file:\n",
    "    for line in file:\n",
    "        counter += 1\n",
    "        listed = line.rstrip().split(\" +++$+++ \") \n",
    "        if len(listed) == 5:\n",
    "            lines.append(listed)\n",
    "            \n",
    "df_movie_lines = pd.DataFrame(np.array(lines),\n",
    "                                columns = ['lineID', 'characterID', 'movieID', 'character name', 'text'])\n",
    "df_movie_lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open characters metadata to DataFrame\n",
    "- movie_characters_metadata.txt\n",
    "\t- contains information about each movie character\n",
    "\t- fields:\n",
    "\t\t- characterID\n",
    "\t\t- character name\n",
    "\t\t- movieID\n",
    "\t\t- movie title\n",
    "\t\t- gender (\"?\" for unlabeled cases)\n",
    "\t\t- position in credits (\"?\" for unlabeled cases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>characterID</th>\n",
       "      <th>character name</th>\n",
       "      <th>movieID</th>\n",
       "      <th>movie title</th>\n",
       "      <th>gender</th>\n",
       "      <th>position in credits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u1</td>\n",
       "      <td>BRUCE</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u2</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>m</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u3</td>\n",
       "      <td>CHASTITY</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u4</td>\n",
       "      <td>JOEY</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>m</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  characterID character name movieID                 movie title gender  \\\n",
       "0          u0         BIANCA      m0  10 things i hate about you      f   \n",
       "1          u1          BRUCE      m0  10 things i hate about you      ?   \n",
       "2          u2        CAMERON      m0  10 things i hate about you      m   \n",
       "3          u3       CHASTITY      m0  10 things i hate about you      ?   \n",
       "4          u4           JOEY      m0  10 things i hate about you      m   \n",
       "\n",
       "  position in credits  \n",
       "0                   4  \n",
       "1                   ?  \n",
       "2                   3  \n",
       "3                   ?  \n",
       "4                   6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load movie characters metadata\n",
    "filename = './ChatBot_Dataset/movie_characters_metadata.txt'\n",
    "counter = 0\n",
    "lines = list()\n",
    "with open(filename) as file:\n",
    "    for line in file:\n",
    "        counter += 1\n",
    "        listed = line.rstrip().split(\" +++$+++ \") \n",
    "        if len(listed) == 6:\n",
    "            lines.append(listed)\n",
    "            \n",
    "df_movie_characters = pd.DataFrame(np.array(lines),\n",
    "                                columns = ['characterID', 'character name', 'movieID', 'movie title', 'gender', 'position in credits'])\n",
    "df_movie_characters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open movie titles metadata to DataFrame\n",
    "- movie_titles_metadata.txt\n",
    "\t- contains information about each movie title\n",
    "\t- fields: \n",
    "\t\t- movieID, \n",
    "\t\t- movie title,\n",
    "\t\t- movie year, \n",
    "\t   \t- IMDB rating,\n",
    "\t\t- no. IMDB votes,\n",
    " \t\t- genres in the format ['genre1','genre2',É,'genreN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieID</th>\n",
       "      <th>movie title</th>\n",
       "      <th>movie year</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>no. IMDB votes</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>1999</td>\n",
       "      <td>6.90</td>\n",
       "      <td>62847</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m1</td>\n",
       "      <td>1492: conquest of paradise</td>\n",
       "      <td>1992</td>\n",
       "      <td>6.20</td>\n",
       "      <td>10421</td>\n",
       "      <td>['adventure', 'biography', 'drama', 'history']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m2</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>2001</td>\n",
       "      <td>6.10</td>\n",
       "      <td>25854</td>\n",
       "      <td>['action', 'crime', 'drama', 'thriller']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m3</td>\n",
       "      <td>2001: a space odyssey</td>\n",
       "      <td>1968</td>\n",
       "      <td>8.40</td>\n",
       "      <td>163227</td>\n",
       "      <td>['adventure', 'mystery', 'sci-fi']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m4</td>\n",
       "      <td>48 hrs.</td>\n",
       "      <td>1982</td>\n",
       "      <td>6.90</td>\n",
       "      <td>22289</td>\n",
       "      <td>['action', 'comedy', 'crime', 'drama', 'thrill...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  movieID                 movie title movie year IMDB rating no. IMDB votes  \\\n",
       "0      m0  10 things i hate about you       1999        6.90          62847   \n",
       "1      m1  1492: conquest of paradise       1992        6.20          10421   \n",
       "2      m2                  15 minutes       2001        6.10          25854   \n",
       "3      m3       2001: a space odyssey       1968        8.40         163227   \n",
       "4      m4                     48 hrs.       1982        6.90          22289   \n",
       "\n",
       "                                              genres  \n",
       "0                              ['comedy', 'romance']  \n",
       "1     ['adventure', 'biography', 'drama', 'history']  \n",
       "2           ['action', 'crime', 'drama', 'thriller']  \n",
       "3                 ['adventure', 'mystery', 'sci-fi']  \n",
       "4  ['action', 'comedy', 'crime', 'drama', 'thrill...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load movie  metadata\n",
    "filename = './ChatBot_Dataset/movie_titles_metadata.txt'\n",
    "counter = 0\n",
    "lines = list()\n",
    "with open(filename) as file:\n",
    "    for line in file:\n",
    "        counter += 1\n",
    "        listed = line.rstrip().split(\" +++$+++ \") \n",
    "        if len(listed) == 6:\n",
    "            lines.append(listed)\n",
    "            \n",
    "df_movie_titles = pd.DataFrame(np.array(lines),\n",
    "                                columns = ['movieID', 'movie title', 'movie year', 'IMDB rating', 'no. IMDB votes', 'genres'])\n",
    "df_movie_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieID</th>\n",
       "      <th>movie title</th>\n",
       "      <th>movie year</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>no. IMDB votes</th>\n",
       "      <th>genres</th>\n",
       "      <th>converted_genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>1999</td>\n",
       "      <td>6.90</td>\n",
       "      <td>62847</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>[comedy, romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m1</td>\n",
       "      <td>1492: conquest of paradise</td>\n",
       "      <td>1992</td>\n",
       "      <td>6.20</td>\n",
       "      <td>10421</td>\n",
       "      <td>['adventure', 'biography', 'drama', 'history']</td>\n",
       "      <td>[adventure, biography, drama, history]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m2</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>2001</td>\n",
       "      <td>6.10</td>\n",
       "      <td>25854</td>\n",
       "      <td>['action', 'crime', 'drama', 'thriller']</td>\n",
       "      <td>[action, crime, drama, thriller]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  movieID                 movie title movie year IMDB rating no. IMDB votes  \\\n",
       "0      m0  10 things i hate about you       1999        6.90          62847   \n",
       "1      m1  1492: conquest of paradise       1992        6.20          10421   \n",
       "2      m2                  15 minutes       2001        6.10          25854   \n",
       "\n",
       "                                           genres  \\\n",
       "0                           ['comedy', 'romance']   \n",
       "1  ['adventure', 'biography', 'drama', 'history']   \n",
       "2        ['action', 'crime', 'drama', 'thriller']   \n",
       "\n",
       "                         converted_genres  \n",
       "0                       [comedy, romance]  \n",
       "1  [adventure, biography, drama, history]  \n",
       "2        [action, crime, drama, thriller]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now \"genres\" column contains all data in string format\n",
    "# decode Genres from str to list\n",
    "def f(row):\n",
    "    # print(row)\n",
    "    return eval(row)\n",
    "    \n",
    "\n",
    "converted_genres = df_movie_titles['genres'].apply(f)\n",
    "df_movie_titles['converted_genres'] = converted_genres\n",
    "df_movie_titles.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will train the model on the conversations from Action films.\n",
    "We can make thematic chat bot :)\n",
    "### Filtering the films with genre \"Action\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['m2', 'm4', 'm5', 'm11', 'm15', 'm19', 'm22', 'm23', 'm25', 'm33',\n",
       "       'm37', 'm39', 'm40', 'm48', 'm58', 'm61', 'm65', 'm66', 'm67',\n",
       "       'm68', 'm74', 'm76', 'm86', 'm89', 'm97', 'm98', 'm99', 'm100',\n",
       "       'm113', 'm122', 'm125', 'm126', 'm129', 'm133', 'm135', 'm142',\n",
       "       'm143', 'm157', 'm169', 'm171', 'm180', 'm185', 'm186', 'm188',\n",
       "       'm189', 'm191', 'm192', 'm193', 'm195', 'm196', 'm197', 'm198',\n",
       "       'm200', 'm206', 'm210', 'm214', 'm220', 'm221', 'm222', 'm226',\n",
       "       'm232', 'm237', 'm248', 'm250', 'm259', 'm260', 'm261', 'm262',\n",
       "       'm271', 'm272', 'm275', 'm282', 'm287', 'm290', 'm291', 'm295',\n",
       "       'm300', 'm303', 'm309', 'm310', 'm311', 'm319', 'm321', 'm323',\n",
       "       'm325', 'm328', 'm333', 'm335', 'm337', 'm339', 'm343', 'm345',\n",
       "       'm351', 'm356', 'm357', 'm366', 'm367', 'm371', 'm377', 'm388',\n",
       "       'm389', 'm392', 'm394', 'm409', 'm410', 'm411', 'm418', 'm420',\n",
       "       'm429', 'm430', 'm433', 'm437', 'm442', 'm443', 'm454', 'm456',\n",
       "       'm457', 'm468', 'm470', 'm472', 'm473', 'm474', 'm477', 'm478',\n",
       "       'm489', 'm492', 'm494', 'm496', 'm497', 'm502', 'm508', 'm521',\n",
       "       'm526', 'm529', 'm530', 'm535', 'm540', 'm541', 'm542', 'm543',\n",
       "       'm544', 'm547', 'm549', 'm570', 'm572', 'm576', 'm577', 'm583',\n",
       "       'm584', 'm585', 'm597', 'm607', 'm608', 'm611', 'm612', 'm613',\n",
       "       'm614', 'm616'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(row):\n",
    "    return 'action' in row\n",
    "\n",
    "action_films = df_movie_titles[  df_movie_titles['converted_genres'].apply(f) ]\n",
    "action_film_tags = action_films.movieID.values\n",
    "action_film_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Dialogues with 2 conversation tags only. Supposed that here will be \"Question - Answer\" chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>characterID_1</th>\n",
       "      <th>characterID_2</th>\n",
       "      <th>movieID</th>\n",
       "      <th>conversation_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>u26</td>\n",
       "      <td>u30</td>\n",
       "      <td>m2</td>\n",
       "      <td>['L3496', 'L3497']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>u26</td>\n",
       "      <td>u30</td>\n",
       "      <td>m2</td>\n",
       "      <td>['L3545', 'L3546']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>u27</td>\n",
       "      <td>u34</td>\n",
       "      <td>m2</td>\n",
       "      <td>['L3096', 'L3097']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>u27</td>\n",
       "      <td>u34</td>\n",
       "      <td>m2</td>\n",
       "      <td>['L3313', 'L3314']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>u27</td>\n",
       "      <td>u34</td>\n",
       "      <td>m2</td>\n",
       "      <td>['L3329', 'L3330']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83087</th>\n",
       "      <td>u9027</td>\n",
       "      <td>u9029</td>\n",
       "      <td>m616</td>\n",
       "      <td>['L666460', 'L666461']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83088</th>\n",
       "      <td>u9027</td>\n",
       "      <td>u9029</td>\n",
       "      <td>m616</td>\n",
       "      <td>['L666485', 'L666486']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83089</th>\n",
       "      <td>u9027</td>\n",
       "      <td>u9029</td>\n",
       "      <td>m616</td>\n",
       "      <td>['L666546', 'L666547']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83093</th>\n",
       "      <td>u9028</td>\n",
       "      <td>u9031</td>\n",
       "      <td>m616</td>\n",
       "      <td>['L666575', 'L666576']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83094</th>\n",
       "      <td>u9030</td>\n",
       "      <td>u9034</td>\n",
       "      <td>m616</td>\n",
       "      <td>['L666256', 'L666257']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10046 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      characterID_1 characterID_2 movieID       conversation_tags\n",
       "301             u26           u30      m2      ['L3496', 'L3497']\n",
       "302             u26           u30      m2      ['L3545', 'L3546']\n",
       "306             u27           u34      m2      ['L3096', 'L3097']\n",
       "314             u27           u34      m2      ['L3313', 'L3314']\n",
       "316             u27           u34      m2      ['L3329', 'L3330']\n",
       "...             ...           ...     ...                     ...\n",
       "83087         u9027         u9029    m616  ['L666460', 'L666461']\n",
       "83088         u9027         u9029    m616  ['L666485', 'L666486']\n",
       "83089         u9027         u9029    m616  ['L666546', 'L666547']\n",
       "83093         u9028         u9031    m616  ['L666575', 'L666576']\n",
       "83094         u9030         u9034    m616  ['L666256', 'L666257']\n",
       "\n",
       "[10046 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def two_replics(conv_tags):\n",
    "    return len(eval(conv_tags)) == 2\n",
    "\n",
    "filtered_replics = df_conversations[ (df_conversations['movieID'].isin(action_film_tags)) & (df_conversations['conversation_tags'].apply(two_replics))]\n",
    "filtered_replics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's convert the conversation_tags into String form and split them by 2 columns. \n",
    "### To Speaker_1 and Speaker_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27961/983116085.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_replics[['speaker_1', 'speaker_2']] = filtered_replics.apply(split_replics, axis=1, result_type ='expand')\n",
      "/tmp/ipykernel_27961/983116085.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_replics[['speaker_1', 'speaker_2']] = filtered_replics.apply(split_replics, axis=1, result_type ='expand')\n"
     ]
    }
   ],
   "source": [
    "def split_replics(row):\n",
    "    \n",
    "    row = eval(row.conversation_tags)\n",
    "\n",
    "    try:\n",
    "        speaker_1 = df_movie_lines[df_movie_lines['lineID'] == row[0]]['text'].values[0]\n",
    "    except IndexError:\n",
    "        speaker_1 = None\n",
    "        \n",
    "    try:\n",
    "        speaker_2 = df_movie_lines[df_movie_lines['lineID'] == row[1]]['text'].values[0]\n",
    "    except IndexError:\n",
    "        speaker_2 = None\n",
    "\n",
    "    return  speaker_1,  speaker_2\n",
    "\n",
    "# tmp = filtered_replics.head(5)\n",
    "# tmp\n",
    "\n",
    "filtered_replics[['speaker_1', 'speaker_2']] = filtered_replics.apply(split_replics, axis=1, result_type ='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if we got any \"NaN\" or \"None\" values, and filter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10046, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_replics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10006, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_replics = filtered_replics.dropna()\n",
    "filtered_replics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "characterID_1        0\n",
       "characterID_2        0\n",
       "movieID              0\n",
       "conversation_tags    0\n",
       "speaker_1            0\n",
       "speaker_2            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_replics.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>characterID_1</th>\n",
       "      <th>characterID_2</th>\n",
       "      <th>movieID</th>\n",
       "      <th>conversation_tags</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>speaker_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>u26</td>\n",
       "      <td>u30</td>\n",
       "      <td>m2</td>\n",
       "      <td>['L3496', 'L3497']</td>\n",
       "      <td>I'm abused.  Don't you think?</td>\n",
       "      <td>I don't think it's abuse, I think it's torture.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>u26</td>\n",
       "      <td>u30</td>\n",
       "      <td>m2</td>\n",
       "      <td>['L3545', 'L3546']</td>\n",
       "      <td>...so we kill someone famous and if we are cau...</td>\n",
       "      <td>Officers, there's your killer, do your duty, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>u27</td>\n",
       "      <td>u34</td>\n",
       "      <td>m2</td>\n",
       "      <td>['L3096', 'L3097']</td>\n",
       "      <td>I can't take you to my place.</td>\n",
       "      <td>Somewhere else?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    characterID_1 characterID_2 movieID   conversation_tags  \\\n",
       "301           u26           u30      m2  ['L3496', 'L3497']   \n",
       "302           u26           u30      m2  ['L3545', 'L3546']   \n",
       "306           u27           u34      m2  ['L3096', 'L3097']   \n",
       "\n",
       "                                             speaker_1  \\\n",
       "301                      I'm abused.  Don't you think?   \n",
       "302  ...so we kill someone famous and if we are cau...   \n",
       "306                      I can't take you to my place.   \n",
       "\n",
       "                                             speaker_2  \n",
       "301    I don't think it's abuse, I think it's torture.  \n",
       "302  Officers, there's your killer, do your duty, a...  \n",
       "306                                    Somewhere else?  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_replics.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whis model will generate output in \"character by character\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm abused.  Don't you think?\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# symbolic chains of the speaker_1\n",
    "input_texts = list(filtered_replics.speaker_1.values) \n",
    "input_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't think it's abuse, I think it's torture.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# symbolic chains of the speaker_2\n",
    "target_texts = list(filtered_replics.speaker_2.values)\n",
    "target_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look at lengths of target symbolic chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   1.,   0.,  49.,  88., 177., 117., 147., 181., 183., 211.,\n",
       "        156., 146., 176., 168., 189., 163., 162., 170., 167., 169., 153.,\n",
       "        174., 158., 173., 160., 148., 150., 148., 153., 149., 158., 135.,\n",
       "        137., 146., 135., 111., 114., 119.,  90.,  90.,  92.,  89., 120.,\n",
       "         98., 100., 101.,  78.,  80.,  82.,  93.,  61.,  87.,  65.,  65.,\n",
       "         69.,  70.,  73.,  79.,  68.,  63.,  76.,  59.,  58.,  47.,  47.,\n",
       "         52.,  55.,  50.,  61.,  54.,  43.,  35.,  34.,  35.,  52.,  44.,\n",
       "         39.,  30.,  45.,  34.,  29.,  31.,  32.,  31.,  32.,  44.,  27.,\n",
       "         34.,  29.,  24.,  33.,  23.,  28.,  22.,  27.,  24.,  33.,  45.]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
       "        13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n",
       "        26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
       "        39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,\n",
       "        52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,\n",
       "        65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,\n",
       "        78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.,\n",
       "        91., 92., 93., 94., 95., 96., 97., 98., 99.]),\n",
       " <BarContainer object of 99 artists>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Оценим длины выходных цепочек\n",
    "seq_length =np.array([len(seq) for seq in target_texts])\n",
    "plt.hist(seq_length, bins = list(range(0, 100,  1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see the symbolic chains length distribution and length of symbolic chain can vary.\n",
    "But we will use chains of fixed length to train our model. \n",
    "That mean we will use \"Padding\" - filling the rest of short symbolic chain with \" \" - symbol.\n",
    "### Important\n",
    "Using the symbolic chains with the Vast majority of Padding character \" \" could cause a problem:\n",
    "Model could generate the sequences of Padding characters, where is no sence.\n",
    "### I will filter only the character-chains where length is not vary too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit_25 19.0\n",
      "limit_75 65.0\n"
     ]
    }
   ],
   "source": [
    "limit_75 = np.percentile(seq_length, 75)\n",
    "limit_25 = np.percentile(seq_length, 25)\n",
    "indices = np.argwhere((seq_length>limit_25) & (seq_length < limit_75))\n",
    "print('limit_25', limit_25)\n",
    "print('limit_75', limit_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of symbolic chains was: 10006\n"
     ]
    }
   ],
   "source": [
    "print('the number of symbolic chains was:', len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering of input and target symbolic chains from indices we've got above\n",
    "input_texts = np.take(input_texts, indices)\n",
    "target_texts = np.take(target_texts, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of symbolic chains after filtering: 4843\n"
     ]
    }
   ],
   "source": [
    "print('the number of symbolic chains after filtering:', len(target_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to LIST type\n",
    "input_texts = [str(elem[0]) for elem in input_texts]\n",
    "target_texts = [str(elem[0]) for elem in target_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of vocabulary\n",
    "we nee to get the list of unique characters from whole texts (speaker_1 + speaker_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = ' '.join(input_texts) + ' ' + ' '.join(target_texts)\n",
    "splitted_all_text = list(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  91\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(splitted_all_text))\n",
    "vocab.append('<START>')\n",
    "vocab.append('<END>')\n",
    "vocab.append(' ')\n",
    "vocab_size = len(vocab)\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "print('vocab_size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts_as_int = [[char2idx[w] for w in text] for text in input_texts]\n",
    "target_texts_as_int = [[char2idx[w] for w in text] for text in target_texts]\n",
    "\n",
    "encoder_input_seqs = [np.array(text) for text in input_texts_as_int]\n",
    "decoder_input_seqs = []\n",
    "decoder_target_seqs = []\n",
    "for target_text in target_texts_as_int:\n",
    "    decoder_input_seqs.append(np.array([char2idx['<START>']] + target_text))\n",
    "    decoder_target_seqs.append(np.array(target_text + [char2idx['<END>']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', \"'\", 's', ' ', 'a', 'b', 'u', 's', 'e', ',', ' ', 'I', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', \"'\", 's', ' ', 't', 'o', 'r', 't', 'u', 'r', 'e', '.']\n"
     ]
    }
   ],
   "source": [
    "print([idx2char[idx] for idx in  decoder_input_seqs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', \"'\", 's', ' ', 'a', 'b', 'u', 's', 'e', ',', ' ', 'I', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', \"'\", 's', ' ', 't', 'o', 'r', 't', 'u', 'r', 'e', '.', '<END>']\n"
     ]
    }
   ],
   "source": [
    "print([idx2char[idx] for idx in  decoder_target_seqs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_enc_seq_length:  524\n",
      "max_dec_seq_length:  65\n"
     ]
    }
   ],
   "source": [
    "max_enc_seq_length = max([len(seq) for seq in encoder_input_seqs])\n",
    "max_dec_seq_length = max([len(seq) for seq in decoder_input_seqs])\n",
    "print('max_enc_seq_length: ', max_enc_seq_length)\n",
    "print('max_dec_seq_length: ', max_dec_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Padding of chains\n",
    "Make all chains the equal length, filling gaps with char2idx[' '] symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_enc_seq_length = 65\n",
    "max_dec_seq_length = 65\n",
    "\n",
    "encoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    encoder_input_seqs,\n",
    "    value=char2idx[' '],\n",
    "    padding='post',\n",
    "    maxlen=max_enc_seq_length)\n",
    "\n",
    "decoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    decoder_input_seqs,\n",
    "    value=char2idx[' '],\n",
    "    padding='post',\n",
    "    maxlen=max_dec_seq_length)\n",
    "\n",
    "decoder_target_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    decoder_target_seqs,\n",
    "    value=char2idx[' '],\n",
    "    padding='post',\n",
    "    maxlen=max_dec_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'm', ' ', 'a', 'b', 'u', 's', 'e', 'd', '.', ' ', ' ', 'D', 'o', 'n', \"'\", 't', ' ', 'y', 'o', 'u', ' ', 't', 'h', 'i', 'n', 'k', '?', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "print([idx2char[idx] for idx in  encoder_input_seqs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', \"'\", 's', ' ', 'a', 'b', 'u', 's', 'e', ',', ' ', 'I', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', \"'\", 's', ' ', 't', 'o', 'r', 't', 'u', 'r', 'e', '.', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "print([idx2char[idx] for idx in  decoder_input_seqs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', \"'\", 's', ' ', 'a', 'b', 'u', 's', 'e', ',', ' ', 'I', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', \"'\", 's', ' ', 't', 'o', 'r', 't', 'u', 'r', 'e', '.', '<END>', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "print([idx2char[idx] for idx in  decoder_target_seqs[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer enc1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer enc1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer enc1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer enc2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer enc2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer enc2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer decoder1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer decoder2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 19:37:39.837622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-11 19:37:39.842181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-11 19:37:39.845814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-11 19:37:40.019391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-11 19:37:40.021330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-11 19:37:40.023024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-11 19:37:40.024674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6643 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "INPUT_VOCAB_SIZE = vocab_size\n",
    "TARGET_VOCAB_SIZE = vocab_size\n",
    "\n",
    "H_SIZE = 256   #64 # size of the hidden state of LSTM\n",
    "EMB_SIZE = H_SIZE * 2  # size of input embedding\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = tf.keras.layers.Embedding(INPUT_VOCAB_SIZE, EMB_SIZE)\n",
    "        self.lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(H_SIZE, return_sequences=True, return_state=True,  recurrent_dropout=0.000001, name = 'encoder1'),\n",
    "            merge_mode=\"concat\"\n",
    "        )\n",
    "        \n",
    "        self.lstm2 =  tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(H_SIZE, return_sequences=False, return_state=True, recurrent_dropout=0.000001, name = 'encoder2'),\n",
    "            merge_mode=\"concat\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embed(x)\n",
    "        out, h_fwd_1, c_fwd_1, h_bwd_1, c_bwd_1 = self.lstm1(out)               \n",
    "        out, h_fwd_2, c_fwd_2, h_bwd_2, c_bwd_2 = self.lstm2(out)\n",
    "\n",
    "        h_1 = tf.keras.layers.Concatenate()([h_fwd_1, h_bwd_1])\n",
    "        c_1 = tf.keras.layers.Concatenate()([c_fwd_1, c_bwd_1])\n",
    "        h_2 = tf.keras.layers.Concatenate()([h_fwd_2, h_bwd_2])\n",
    "        c_2 = tf.keras.layers.Concatenate()([c_fwd_2, c_bwd_2])  \n",
    "\n",
    "        state1 = [h_1, c_1]\n",
    "        state2 = [h_2, c_2]\n",
    "\n",
    "        return state1 , state2\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = tf.keras.layers.Embedding(TARGET_VOCAB_SIZE, EMB_SIZE)\n",
    "\n",
    "        self.lstm1 = tf.keras.layers.LSTM(2*H_SIZE, return_sequences=True, return_state=True, recurrent_dropout=0.000001, name = 'decoder1')\n",
    "        self.lstm2 = tf.keras.layers.LSTM(2*H_SIZE, return_sequences=True, return_state=True, recurrent_dropout=0.000001, name = 'decoder2')\n",
    "        self.fc = tf.keras.layers.Dense(TARGET_VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "    def call(self, x, init_state1, init_state2):\n",
    "        out = self.embed(x)\n",
    "        \n",
    "        out, h_1, c_1 = self.lstm1(out, initial_state=init_state1)        \n",
    "        out, h_2, c_2 = self.lstm1(out, initial_state=init_state2)\n",
    "        out = self.fc(out)\n",
    "        state1 = [h_1, c_1]\n",
    "        state2 = [h_2, c_2]\n",
    "        \n",
    "        return out, state1, state2\n",
    "\n",
    "encoder_model = Encoder()\n",
    "decoder_model = Decoder()\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "\n",
    "enc_state1,  enc_state2 = encoder_model(encoder_inputs)\n",
    "decoder_outputs, _ , _ = decoder_model(decoder_inputs, enc_state1, enc_state2)\n",
    "\n",
    "seq2seq = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "76/76 [==============================] - 51s 552ms/step - loss: 0.1313 - accuracy: 0.9627\n",
      "Epoch 2/100\n",
      "76/76 [==============================] - 41s 537ms/step - loss: 0.1265 - accuracy: 0.9638\n",
      "Epoch 3/100\n",
      "76/76 [==============================] - 41s 537ms/step - loss: 0.1235 - accuracy: 0.9648\n",
      "Epoch 4/100\n",
      "76/76 [==============================] - 41s 533ms/step - loss: 0.1216 - accuracy: 0.9653\n",
      "Epoch 5/100\n",
      "76/76 [==============================] - 41s 539ms/step - loss: 0.1189 - accuracy: 0.9661\n",
      "Epoch 6/100\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.1168 - accuracy: 0.9668\n",
      "Epoch 7/100\n",
      "76/76 [==============================] - 41s 536ms/step - loss: 0.1232 - accuracy: 0.9657\n",
      "Epoch 8/100\n",
      "76/76 [==============================] - 41s 536ms/step - loss: 0.1128 - accuracy: 0.9679\n",
      "Epoch 9/100\n",
      "76/76 [==============================] - 41s 541ms/step - loss: 0.1097 - accuracy: 0.9688\n",
      "Epoch 10/100\n",
      "76/76 [==============================] - 41s 541ms/step - loss: 0.1073 - accuracy: 0.9696\n",
      "Epoch 11/100\n",
      "76/76 [==============================] - 41s 536ms/step - loss: 0.1050 - accuracy: 0.9701\n",
      "Epoch 12/100\n",
      "76/76 [==============================] - 41s 541ms/step - loss: 0.1021 - accuracy: 0.9710\n",
      "Epoch 13/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0999 - accuracy: 0.9718\n",
      "Epoch 14/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0979 - accuracy: 0.9724\n",
      "Epoch 15/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0955 - accuracy: 0.9732\n",
      "Epoch 16/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0929 - accuracy: 0.9738\n",
      "Epoch 17/100\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.0907 - accuracy: 0.9746\n",
      "Epoch 18/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0893 - accuracy: 0.9752\n",
      "Epoch 19/100\n",
      "76/76 [==============================] - 41s 534ms/step - loss: 0.0868 - accuracy: 0.9762\n",
      "Epoch 20/100\n",
      "76/76 [==============================] - 41s 534ms/step - loss: 0.0844 - accuracy: 0.9767\n",
      "Epoch 21/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0824 - accuracy: 0.9772\n",
      "Epoch 22/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0806 - accuracy: 0.9779\n",
      "Epoch 23/100\n",
      "76/76 [==============================] - 41s 533ms/step - loss: 0.0785 - accuracy: 0.9786\n",
      "Epoch 24/100\n",
      "76/76 [==============================] - 41s 538ms/step - loss: 0.0764 - accuracy: 0.9794\n",
      "Epoch 25/100\n",
      "76/76 [==============================] - 41s 538ms/step - loss: 0.0745 - accuracy: 0.9796\n",
      "Epoch 26/100\n",
      "76/76 [==============================] - 41s 534ms/step - loss: 0.0723 - accuracy: 0.9805\n",
      "Epoch 27/100\n",
      "76/76 [==============================] - 41s 539ms/step - loss: 0.0700 - accuracy: 0.9812\n",
      "Epoch 28/100\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.0681 - accuracy: 0.9818\n",
      "Epoch 29/100\n",
      "76/76 [==============================] - 41s 545ms/step - loss: 0.0661 - accuracy: 0.9826\n",
      "Epoch 30/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0646 - accuracy: 0.9827\n",
      "Epoch 31/100\n",
      "76/76 [==============================] - 41s 534ms/step - loss: 0.0624 - accuracy: 0.9835\n",
      "Epoch 32/100\n",
      "76/76 [==============================] - 41s 544ms/step - loss: 0.0605 - accuracy: 0.9842\n",
      "Epoch 33/100\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.0587 - accuracy: 0.9847\n",
      "Epoch 34/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0572 - accuracy: 0.9851\n",
      "Epoch 35/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0555 - accuracy: 0.9857\n",
      "Epoch 36/100\n",
      "76/76 [==============================] - 41s 533ms/step - loss: 0.0532 - accuracy: 0.9864\n",
      "Epoch 37/100\n",
      "76/76 [==============================] - 41s 537ms/step - loss: 0.0517 - accuracy: 0.9868\n",
      "Epoch 38/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0500 - accuracy: 0.9874\n",
      "Epoch 39/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0486 - accuracy: 0.9877\n",
      "Epoch 40/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0466 - accuracy: 0.9884\n",
      "Epoch 41/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0451 - accuracy: 0.9889\n",
      "Epoch 42/100\n",
      "76/76 [==============================] - 41s 538ms/step - loss: 0.0436 - accuracy: 0.9893\n",
      "Epoch 43/100\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.0419 - accuracy: 0.9898\n",
      "Epoch 44/100\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.0402 - accuracy: 0.9903\n",
      "Epoch 45/100\n",
      "76/76 [==============================] - 41s 538ms/step - loss: 0.0387 - accuracy: 0.9909\n",
      "Epoch 46/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0373 - accuracy: 0.9913\n",
      "Epoch 47/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0356 - accuracy: 0.9917\n",
      "Epoch 48/100\n",
      "76/76 [==============================] - 41s 539ms/step - loss: 0.0341 - accuracy: 0.9922\n",
      "Epoch 49/100\n",
      "76/76 [==============================] - 41s 536ms/step - loss: 0.0326 - accuracy: 0.9926\n",
      "Epoch 50/100\n",
      "76/76 [==============================] - 41s 534ms/step - loss: 0.0312 - accuracy: 0.9930\n",
      "Epoch 51/100\n",
      "76/76 [==============================] - 41s 533ms/step - loss: 0.0294 - accuracy: 0.9937\n",
      "Epoch 52/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0280 - accuracy: 0.9940\n",
      "Epoch 53/100\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.0267 - accuracy: 0.9943\n",
      "Epoch 54/100\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.0252 - accuracy: 0.9948\n",
      "Epoch 55/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0244 - accuracy: 0.9951\n",
      "Epoch 56/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0226 - accuracy: 0.9954\n",
      "Epoch 57/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0463 - accuracy: 0.9882\n",
      "Epoch 58/100\n",
      "76/76 [==============================] - 41s 534ms/step - loss: 0.0739 - accuracy: 0.9804\n",
      "Epoch 59/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0699 - accuracy: 0.9821\n",
      "Epoch 60/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0605 - accuracy: 0.9854\n",
      "Epoch 61/100\n",
      "76/76 [==============================] - 41s 538ms/step - loss: 0.0536 - accuracy: 0.9882\n",
      "Epoch 62/100\n",
      "76/76 [==============================] - 42s 549ms/step - loss: 0.0463 - accuracy: 0.9904\n",
      "Epoch 63/100\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.0394 - accuracy: 0.9926\n",
      "Epoch 64/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0322 - accuracy: 0.9945\n",
      "Epoch 65/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0284 - accuracy: 0.9955\n",
      "Epoch 66/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0226 - accuracy: 0.9965\n",
      "Epoch 67/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0181 - accuracy: 0.9970\n",
      "Epoch 68/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0159 - accuracy: 0.9973\n",
      "Epoch 69/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0140 - accuracy: 0.9974\n",
      "Epoch 70/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0129 - accuracy: 0.9977\n",
      "Epoch 71/100\n",
      "76/76 [==============================] - 41s 534ms/step - loss: 0.0122 - accuracy: 0.9978\n",
      "Epoch 72/100\n",
      "76/76 [==============================] - 40s 529ms/step - loss: 0.0115 - accuracy: 0.9979\n",
      "Epoch 73/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0109 - accuracy: 0.9979\n",
      "Epoch 74/100\n",
      "76/76 [==============================] - 41s 536ms/step - loss: 0.0105 - accuracy: 0.9980\n",
      "Epoch 75/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0102 - accuracy: 0.9980\n",
      "Epoch 76/100\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.0097 - accuracy: 0.9981\n",
      "Epoch 77/100\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.0093 - accuracy: 0.9982\n",
      "Epoch 78/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0089 - accuracy: 0.9983\n",
      "Epoch 79/100\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.0086 - accuracy: 0.9983\n",
      "Epoch 80/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0083 - accuracy: 0.9984\n",
      "Epoch 81/100\n",
      "76/76 [==============================] - 40s 533ms/step - loss: 0.0080 - accuracy: 0.9985\n",
      "Epoch 82/100\n",
      "76/76 [==============================] - 41s 536ms/step - loss: 0.0077 - accuracy: 0.9985\n",
      "Epoch 83/100\n",
      "76/76 [==============================] - 41s 539ms/step - loss: 0.0076 - accuracy: 0.9985\n",
      "Epoch 84/100\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0074 - accuracy: 0.9985\n",
      "Epoch 85/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0071 - accuracy: 0.9986\n",
      "Epoch 86/100\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.0069 - accuracy: 0.9986\n",
      "Epoch 87/100\n",
      "76/76 [==============================] - 41s 534ms/step - loss: 0.0067 - accuracy: 0.9986\n",
      "Epoch 88/100\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.0065 - accuracy: 0.9987\n",
      "Epoch 89/100\n",
      "76/76 [==============================] - 41s 533ms/step - loss: 0.0063 - accuracy: 0.9987\n",
      "Epoch 90/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0062 - accuracy: 0.9988\n",
      "Epoch 91/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 92/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 93/100\n",
      "76/76 [==============================] - 41s 533ms/step - loss: 0.0058 - accuracy: 0.9988\n",
      "Epoch 94/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0056 - accuracy: 0.9988\n",
      "Epoch 95/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 96/100\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 97/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0053 - accuracy: 0.9989\n",
      "Epoch 98/100\n",
      "76/76 [==============================] - 40s 532ms/step - loss: 0.0053 - accuracy: 0.9989\n",
      "Epoch 99/100\n",
      "76/76 [==============================] - 41s 541ms/step - loss: 0.0050 - accuracy: 0.9989\n",
      "Epoch 100/100\n",
      "76/76 [==============================] - 41s 535ms/step - loss: 0.0049 - accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fdc44157580>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy()\n",
    "seq2seq.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])\n",
    "seq2seq.fit([encoder_input_seqs, decoder_input_seqs], decoder_target_seqs,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_inference(input_seq):\n",
    "    state1, state2 = encoder_model(input_seq)\n",
    "\n",
    "    target_seq = np.array([[char2idx['<START>']]])\n",
    "\n",
    "    decoded_sentence = ''\n",
    "    while True:\n",
    "        output_tokens, state1, state2 = decoder_model(target_seq, state1, state2)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :]) \n",
    "        sampled_char = idx2char[sampled_token_index]\n",
    "        \n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '<END>' or len(decoded_sentence) > 100):\n",
    "            break\n",
    "\n",
    "        target_seq = np.array([[sampled_token_index]])\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define the function of user inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Let's test what you have learned\n",
      "Result sentence: He's a moming big ling.sse fat anything.<END>\n"
     ]
    }
   ],
   "source": [
    "def user_inference(user_text):\n",
    "    user_seq = np.array([[char2idx[c] for c in user_text]], dtype='int32')\n",
    "    decoded_sentence = seq2seq_inference(user_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', user_text)\n",
    "    print('Result sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make some tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Hello! Show me please what have you learned today!\n",
      "Result sentence: I don't care.  Hank.<END>\n"
     ]
    }
   ],
   "source": [
    "user_inference(\"Hello! Show me please what have you learned today!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: tell me about yourself\n",
      "Result sentence: I don't think so, Detective ...<END>\n"
     ]
    }
   ],
   "source": [
    "user_inference(\"tell me about yourself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: it is up to you actually. But I suppose that would be better for you if you speak\n",
      "Result sentence: Yeah.  Yeah, that'll work.<END>\n"
     ]
    }
   ],
   "source": [
    "user_inference(\"it is up to you actually. But I suppose that would be better for you if you speak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: great! So who have yo spoken with before I come?\n",
      "Result sentence: I was just thinking that.<END>\n"
     ]
    }
   ],
   "source": [
    "user_inference(\"great! So who have yo spoken with before I come?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Could you finish your every sentence please? As I see you are not going to speak a lot today, right?\n",
      "Result sentence: What are they doing in the middle of nowhere?<END>\n"
     ]
    }
   ],
   "source": [
    "user_inference(\"Could you finish your every sentence please? As I see you are not going to speak a lot today, right?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Who you are speaking about?\n",
      "Result sentence: You don't set money.<END>\n"
     ]
    }
   ],
   "source": [
    "user_inference(\"Who you are speaking about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: I did, you forgot right?\n",
      "Result sentence: The door.  Let get one of the beat.<END>\n"
     ]
    }
   ],
   "source": [
    "user_inference(\"I did, you forgot right?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
