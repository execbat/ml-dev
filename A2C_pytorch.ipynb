{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2559ac4-ec41-4e31-92f4-2e36d5f19733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "264c5b06-202e-4b30-9830-8e2621f1fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_obs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_obs):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_obs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b924a75-8412-4aa9-a0cc-e3aeab60ee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "n_obs = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(n_obs)\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "941ec87e-39ab-4fba-a3f2-a9aab3101ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "\n",
    "actor = Actor(n_obs, n_actions)\n",
    "critic = Critic(n_obs)\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr = LR)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0173a48-4060-4fbd-8c84-bbcd8f13990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C training after single episode\n",
    "def get_returns(rewards, gamma):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    return returns\n",
    "\n",
    "def train(env, actor, critic, optimizer_actor, optimizer_critic, episodes = 5000, gamma = 0.99):\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        log_probs, states, rewards = [], [], []\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # actor works\n",
    "            state_tensor = torch.tensor(state, dtype = torch.float32)\n",
    "            action_probs = actor(state_tensor.unsqueeze(0))\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)            \n",
    "            \n",
    "            log_probs.append(log_prob)  \n",
    "            states.append(state_tensor) # collect states to estimate Q-values by the Critic after\n",
    "            \n",
    "            # make new step\n",
    "            state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "\n",
    "            rewards.append(reward)      # collect rewards for calculation of the discounted returns by gamma\n",
    "\n",
    "        # train after episode ends\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "        # calculate cumulative reward for every step\n",
    "        returns = get_returns(rewards, gamma)\n",
    "        returns_tensor = torch.tensor(returns, dtype = torch.float32)\n",
    "\n",
    "        # get Q-values of these states by Critic\n",
    "        values = critic(torch.stack(states)).squeeze(-1)    \n",
    "\n",
    "        # train Critic\n",
    "        loss_critic = nn.MSELoss()(values, returns_tensor)\n",
    "        optimizer_critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # calc advantages considering feedback from Critic\n",
    "        advantages = returns_tensor - values.detach()\n",
    "        # advantages = (advantages - advantages.mean()) / (advantages.std() + 5e-20)\n",
    "\n",
    "        # train Actor\n",
    "        actor_loss = - torch.sum(torch.stack(log_probs) * advantages)\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        if episode > 0 and episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, mean reward {sum(total_rewards[-100:]) / 100.0}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d0eff4-b4d8-4803-af47-f61b9ec32bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, mean reward 29.18\n",
      "Episode 200, mean reward 39.17\n",
      "Episode 300, mean reward 58.06\n",
      "Episode 400, mean reward 70.43\n",
      "Episode 500, mean reward 83.29\n",
      "Episode 600, mean reward 181.84\n",
      "Episode 700, mean reward 255.83\n",
      "Episode 800, mean reward 319.47\n",
      "Episode 900, mean reward 424.95\n",
      "Episode 1000, mean reward 414.86\n",
      "Episode 1100, mean reward 440.95\n",
      "Episode 1200, mean reward 408.31\n",
      "Episode 1300, mean reward 453.25\n",
      "Episode 1400, mean reward 414.94\n",
      "Episode 1500, mean reward 433.86\n",
      "Episode 1600, mean reward 422.0\n",
      "Episode 1700, mean reward 445.14\n",
      "Episode 1800, mean reward 457.16\n",
      "Episode 1900, mean reward 441.15\n",
      "Episode 2000, mean reward 468.06\n",
      "Episode 2100, mean reward 458.43\n",
      "Episode 2200, mean reward 484.11\n",
      "Episode 2300, mean reward 467.41\n",
      "Episode 2400, mean reward 441.16\n",
      "Episode 2500, mean reward 470.37\n",
      "Episode 2600, mean reward 380.93\n",
      "Episode 2700, mean reward 456.14\n",
      "Episode 2800, mean reward 454.27\n",
      "Episode 2900, mean reward 443.71\n",
      "Episode 3000, mean reward 477.98\n",
      "Episode 3100, mean reward 486.03\n",
      "Episode 3200, mean reward 463.36\n",
      "Episode 3300, mean reward 447.39\n",
      "Episode 3400, mean reward 357.16\n",
      "Episode 3500, mean reward 260.8\n",
      "Episode 3600, mean reward 490.2\n",
      "Episode 3700, mean reward 465.09\n",
      "Episode 3800, mean reward 462.23\n",
      "Episode 3900, mean reward 494.79\n",
      "Episode 4000, mean reward 453.74\n",
      "Episode 4100, mean reward 451.33\n",
      "Episode 4200, mean reward 480.88\n",
      "Episode 4300, mean reward 456.81\n",
      "Episode 4400, mean reward 447.8\n",
      "Episode 4500, mean reward 423.04\n",
      "Episode 4600, mean reward 488.29\n",
      "Episode 4700, mean reward 495.19\n",
      "Episode 4800, mean reward 495.61\n",
      "Episode 4900, mean reward 496.69\n"
     ]
    }
   ],
   "source": [
    "train(env, actor, critic, optimizer_actor, optimizer_critic, episodes = 5000, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60bba27e-88c1-4c51-ad85-f20433804415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C training with batches\n",
    "def get_returns(rewards, gamma):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    return returns\n",
    "\n",
    "def train(env, actor, critic, optimizer_actor, optimizer_critic, episodes = 5000, gamma = 0.99, batch_size = 32):    \n",
    "    total_rewards, batch_log_probs, batch_states, batch_returns = [], [], [], []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        log_probs, states, rewards = [], [], []      \n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # actor works\n",
    "            state_tensor = torch.tensor(state, dtype = torch.float32)\n",
    "            action_probs = actor(state_tensor.unsqueeze(0))\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)          \n",
    "            \n",
    "            log_probs.append(log_prob)  \n",
    "            states.append(state_tensor) # collect states to estimate Q-values by the Critic after\n",
    "            \n",
    "            # make new step\n",
    "            state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "\n",
    "            rewards.append(reward)      # collect rewards for calculation of the discounted returns by gamma\n",
    "\n",
    "        # EPISODE ENDS\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "        # calculate cumulative reward for every step\n",
    "        returns = get_returns(rewards, gamma)\n",
    "        returns_tensor = torch.tensor(returns, dtype = torch.float32)\n",
    "\n",
    "        # COLLECT DATA AFTER EPISODE\n",
    "        batch_log_probs.extend(log_probs)\n",
    "        batch_states.extend(states)\n",
    "        batch_returns.extend(returns_tensor)\n",
    "\n",
    "        # TRAIN\n",
    "        if len(batch_log_probs) >= batch_size:\n",
    "            # print(\"training...\")\n",
    "            # shuffling\n",
    "            idxs = torch.randperm(batch_size)\n",
    "            states_batch = torch.stack(batch_states)[idxs]\n",
    "            log_probs_batch = torch.stack(batch_log_probs)[idxs]\n",
    "            returns_batch = torch.tensor(batch_returns)[idxs]            \n",
    "            \n",
    "            # get Q-values of these states by Critic\n",
    "            values = critic(states_batch).squeeze(-1) \n",
    "    \n",
    "            # train Critic\n",
    "            loss_critic = nn.MSELoss()(values, returns_batch)\n",
    "            optimizer_critic.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            optimizer_critic.step()\n",
    "    \n",
    "            # calc advantages considering feedback from Critic\n",
    "            advantages = returns_batch - values.detach()\n",
    "            # advantages = (advantages - advantages.mean()) / (advantages.std() + 5e-20)\n",
    "    \n",
    "            # train Actor\n",
    "            actor_loss = - torch.sum(log_probs_batch * advantages)\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "\n",
    "            # RESET MEMORY\n",
    "            batch_log_probs = []\n",
    "            batch_states = []    \n",
    "            batch_returns = []\n",
    "\n",
    "        if episode > 0 and episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, mean reward {sum(total_rewards[-100:]) / 100.0}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec84e724-53dd-46c7-9af4-626590b8906e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, mean reward 22.69\n",
      "Episode 200, mean reward 28.15\n",
      "Episode 300, mean reward 31.76\n",
      "Episode 400, mean reward 34.4\n",
      "Episode 500, mean reward 38.61\n",
      "Episode 600, mean reward 40.12\n",
      "Episode 700, mean reward 45.88\n",
      "Episode 800, mean reward 70.23\n",
      "Episode 900, mean reward 106.04\n",
      "Episode 1000, mean reward 166.62\n",
      "Episode 1100, mean reward 229.99\n",
      "Episode 1200, mean reward 283.56\n",
      "Episode 1300, mean reward 368.27\n",
      "Episode 1400, mean reward 415.42\n",
      "Episode 1500, mean reward 389.66\n",
      "Episode 1600, mean reward 417.35\n",
      "Episode 1700, mean reward 441.54\n",
      "Episode 1800, mean reward 445.58\n",
      "Episode 1900, mean reward 461.41\n",
      "Episode 2000, mean reward 388.51\n",
      "Episode 2100, mean reward 432.87\n",
      "Episode 2200, mean reward 437.05\n",
      "Episode 2300, mean reward 459.05\n",
      "Episode 2400, mean reward 454.8\n",
      "Episode 2500, mean reward 470.0\n",
      "Episode 2600, mean reward 475.3\n",
      "Episode 2700, mean reward 474.15\n",
      "Episode 2800, mean reward 483.9\n",
      "Episode 2900, mean reward 483.02\n",
      "Episode 3000, mean reward 486.85\n",
      "Episode 3100, mean reward 489.27\n",
      "Episode 3200, mean reward 486.65\n",
      "Episode 3300, mean reward 497.57\n",
      "Episode 3400, mean reward 486.2\n",
      "Episode 3500, mean reward 493.18\n",
      "Episode 3600, mean reward 487.45\n",
      "Episode 3700, mean reward 494.9\n",
      "Episode 3800, mean reward 457.08\n",
      "Episode 3900, mean reward 424.18\n",
      "Episode 4000, mean reward 474.43\n",
      "Episode 4100, mean reward 497.38\n",
      "Episode 4200, mean reward 492.67\n",
      "Episode 4300, mean reward 500.0\n",
      "Episode 4400, mean reward 496.83\n",
      "Episode 4500, mean reward 496.93\n",
      "Episode 4600, mean reward 498.5\n",
      "Episode 4700, mean reward 495.09\n",
      "Episode 4800, mean reward 496.98\n",
      "Episode 4900, mean reward 498.8\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "actor = Actor(n_obs, n_actions)\n",
    "critic = Critic(n_obs)\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr = LR)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr = LR)\n",
    "\n",
    "train(env, actor, critic, optimizer_actor, optimizer_critic, episodes = 5000, gamma = 0.99, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a08f82-d4b8-447d-bc2a-b14ac1d4a65d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_Python3.9.18",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
