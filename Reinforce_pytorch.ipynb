{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72867f76-6207-4c32-9e49-434dcec87683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a19a184-d5f7-4ecd-a18f-b01cae31d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        super().__init__()\n",
    "        self.stream = nn.Sequential(\n",
    "            nn.Linear(n_obs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stream(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2b58da-375e-4905-b4ec-f7b9d25d319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "n_obs = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13fe3ee8-6823-44ef-bfcb-c57de0dcd7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 reward 26.0\n",
      "Episode 10 reward 64.0\n",
      "Episode 20 reward 60.0\n",
      "Episode 30 reward 43.0\n",
      "Episode 40 reward 101.0\n",
      "Episode 50 reward 26.0\n",
      "Episode 60 reward 24.0\n",
      "Episode 70 reward 26.0\n",
      "Episode 80 reward 58.0\n",
      "Episode 90 reward 60.0\n",
      "Episode 100 reward 82.0\n",
      "Episode 110 reward 34.0\n",
      "Episode 120 reward 28.0\n",
      "Episode 130 reward 27.0\n",
      "Episode 140 reward 108.0\n",
      "Episode 150 reward 124.0\n",
      "Episode 160 reward 61.0\n",
      "Episode 170 reward 60.0\n",
      "Episode 180 reward 128.0\n",
      "Episode 190 reward 87.0\n",
      "Episode 200 reward 47.0\n",
      "Episode 210 reward 87.0\n",
      "Episode 220 reward 310.0\n",
      "Episode 230 reward 158.0\n",
      "Episode 240 reward 262.0\n",
      "Episode 250 reward 500.0\n",
      "Episode 260 reward 270.0\n",
      "Episode 270 reward 500.0\n",
      "Episode 280 reward 500.0\n",
      "Episode 290 reward 500.0\n",
      "Episode 300 reward 432.0\n",
      "Episode 310 reward 154.0\n",
      "Episode 320 reward 150.0\n",
      "Episode 330 reward 248.0\n",
      "Episode 340 reward 99.0\n",
      "Episode 350 reward 157.0\n",
      "Episode 360 reward 500.0\n",
      "Episode 370 reward 500.0\n",
      "Episode 380 reward 221.0\n",
      "Episode 390 reward 137.0\n",
      "Episode 400 reward 120.0\n",
      "Episode 410 reward 77.0\n",
      "Episode 420 reward 67.0\n",
      "Episode 430 reward 82.0\n",
      "Episode 440 reward 75.0\n",
      "Episode 450 reward 84.0\n",
      "Episode 460 reward 114.0\n",
      "Episode 470 reward 124.0\n",
      "Episode 480 reward 168.0\n",
      "Episode 490 reward 121.0\n"
     ]
    }
   ],
   "source": [
    "# Train every episode\n",
    "def get_episode_Gt(rewards, gamma):\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def train(env, policy, optimizer, episodes = 500, gamma = 0.99):    \n",
    "    for episode in range(episodes):\n",
    "        rewards, log_probs = [], []\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        done = False\n",
    "        while not done: # play episode\n",
    "\n",
    "            state_tensor = torch.tensor(state, dtype = torch.float32)\n",
    "            action_probs = policy(state_tensor)\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            log_prob = dist.log_prob(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            rewards.append(reward)\n",
    "\n",
    "        # learn\n",
    "        # prepare returns\n",
    "        returns =  get_episode_Gt(rewards, gamma)   \n",
    "        returns_tensor = torch.tensor(returns, dtype = torch.float32)\n",
    "        returns_tensor = (returns_tensor - returns_tensor.mean()) / (returns_tensor.std() + 3e-20)\n",
    "\n",
    "        # prepare log_probs        \n",
    "        loss = - torch.sum(torch.stack(log_probs) * returns_tensor)\n",
    "\n",
    "        # train\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode} reward {sum(rewards)}\")\n",
    "\n",
    "          \n",
    "LR = 0.001\n",
    "\n",
    "policyNN = Policy(n_obs, n_actions)            \n",
    "optimizer = optim.Adam(policyNN.parameters(), lr = LR)        \n",
    "train(env, policy = policyNN, optimizer = optimizer, episodes = 5000, gamma = 0.99)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc582dbd-efb8-4c0b-9a71-7dac1d60062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 mean reward over last episodes 0.11\n",
      "Episode 100 mean reward over last episodes 52.3\n",
      "Episode 200 mean reward over last episodes 156.57\n",
      "Episode 300 mean reward over last episodes 207.28\n",
      "Episode 400 mean reward over last episodes 165.35\n",
      "Episode 500 mean reward over last episodes 188.67\n",
      "Episode 600 mean reward over last episodes 240.21\n",
      "Episode 700 mean reward over last episodes 437.65\n",
      "Episode 800 mean reward over last episodes 319.8\n",
      "Episode 900 mean reward over last episodes 349.62\n",
      "Episode 1000 mean reward over last episodes 288.92\n",
      "Episode 1100 mean reward over last episodes 500.0\n",
      "Episode 1200 mean reward over last episodes 495.64\n",
      "Episode 1300 mean reward over last episodes 468.34\n",
      "Episode 1400 mean reward over last episodes 440.25\n",
      "Episode 1500 mean reward over last episodes 495.7\n",
      "Episode 1600 mean reward over last episodes 446.84\n",
      "Episode 1700 mean reward over last episodes 494.43\n",
      "Episode 1800 mean reward over last episodes 487.78\n",
      "Episode 1900 mean reward over last episodes 471.02\n",
      "Episode 2000 mean reward over last episodes 394.45\n",
      "Episode 2100 mean reward over last episodes 500.0\n",
      "Episode 2200 mean reward over last episodes 387.14\n",
      "Episode 2300 mean reward over last episodes 81.21\n",
      "Episode 2400 mean reward over last episodes 424.09\n",
      "Episode 2500 mean reward over last episodes 500.0\n",
      "Episode 2600 mean reward over last episodes 500.0\n",
      "Episode 2700 mean reward over last episodes 500.0\n",
      "Episode 2800 mean reward over last episodes 166.0\n",
      "Episode 2900 mean reward over last episodes 132.62\n"
     ]
    }
   ],
   "source": [
    "# Train batch of episodes\n",
    "def get_episode_Gt(rewards, gamma):\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def train(env, policy, optimizer, episodes = 500, gamma = 0.99, batch_size = 512):\n",
    "    batch_log_probs, batch_returns, total_rewards = [], [], []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        rewards, log_probs = [], []\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        done = False\n",
    "        while not done: # play episode\n",
    "\n",
    "            state_tensor = torch.tensor(state, dtype = torch.float32)\n",
    "            action_probs = policy(state_tensor)\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            log_prob = dist.log_prob(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            rewards.append(reward)\n",
    "\n",
    "        # calculate returns over episode\n",
    "        returns =  get_episode_Gt(rewards, gamma)  \n",
    "\n",
    "        # collect log_probs and rewards\n",
    "        batch_log_probs.extend(log_probs)\n",
    "        batch_returns.extend(returns)\n",
    "        total_rewards.append(sum(rewards))\n",
    "        \n",
    "        # learn\n",
    "        if len(batch_log_probs) >= batch_size:      \n",
    "            # prepare returns\n",
    "            \n",
    "            returns_tensor = torch.tensor(batch_returns, dtype = torch.float32)\n",
    "            returns_tensor = (returns_tensor - returns_tensor.mean()) / (returns_tensor.std() + 3e-20)\n",
    "\n",
    "            # stack batch_log_probs into a single tensor and compute loss. Since we need gradient ascent - we use \"-\". The loss - actually expected cumulative reward here\n",
    "            loss = - torch.sum(torch.stack(batch_log_probs) * returns_tensor)\n",
    "    \n",
    "            # train\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # reset batch\n",
    "            batch_log_probs, batch_returns = [], []\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode} mean reward over last episodes {sum(total_rewards[-100:]) / 100.0}\")\n",
    "\n",
    "       \n",
    "BATCH_SIZE = 512            \n",
    "LR = 0.001\n",
    "\n",
    "policyNN = Policy(n_obs, n_actions)            \n",
    "optimizer = optim.Adam(policyNN.parameters(), lr = LR)        \n",
    "train(env, policy = policyNN, optimizer = optimizer, episodes = 3000, gamma = 0.99, batch_size = BATCH_SIZE)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_Python3.9.18",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
